# JEDaI

## Introduction

JEDaI is my attempt to create a silicon-based lifeform that attempts to replicate how carbon-based lifeforms grow and evolve.

## How to use

1. Install the dependencies. Most of the dependencies are python libraries.
2. Run the script (if exists) or run the python file.

## Versioning

The age of JEDaI would be represented in the version number.
Age 0: The JEDaI is born. Exploring the controls.
Age 1: The JEDaI can see, listen, move, and talk.
Age 2: The JEDaI can learn, remember, and understand.
Age 3: The JEDaI can feel, smell, taste, and interact with the world.
Age 4: The JEDaI can make decisions and act accordingly.

## How it works

### Audio (output)

The audio output would be generated using gpt/llama/gemini models.

### Video (input)

The video input would be taken from a webcam. The video would be processed frame by frame.

### Movements (output)

The movements would be generated based on the system. At the moment, the movements are outputs like triggering of lights, playing music, etc.
Later, the robotic arm would be added to the system. 

### Context (inference)

The context would be generated using sensors and camera. The aim is to make JEDaI able to see, listen, feel, smell, and taste.
The context format would be JSON. The gpt/llama/gemini models would be used to generate the relavant context.

### Learning and Knowledge

The information would be extracted from wikipedia, gpts, and other sources.
Knowledge extraction would be done using gpt o1 model.

### Memory

The memory would be stored in the form of JSON. The memory would be used to store the context, the audio, the video, the movements, etc.

### Personality and Emotions

The personality would be generated using 'experiences'. The experiences would be generated using the sensors and camera. And how people interact with JEDaI would be used to generate the personality.
The emotions would be generated using past 'experiences', 'personality', and current situation.

### Actions (Reactions)

The actions would be triggered using 'emotions', 'personality', and current situation.

### Interactions

The interactions would be done using the audio, video, movements, and dashboards.

### Use [JEDaI forum to discuss](https://github.com/JEDaI/JEDaI) (to be created)

